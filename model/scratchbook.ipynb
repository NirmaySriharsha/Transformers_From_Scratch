{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer import Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script to build transformers\"\"\"\n",
    "\n",
    "\n",
    "def build_transformer(default=True):\n",
    "    if not default:\n",
    "        raise NotImplementedError\n",
    "    vocab_size = 10000\n",
    "    embedding_dims = 512\n",
    "    num_encoders = 6\n",
    "    num_decoders = 6\n",
    "    num_heads = 8\n",
    "    dropout = 0.1\n",
    "    expansion_dims = 2048\n",
    "    sequence_length = 100\n",
    "    source_embedding_layer_params = {\n",
    "        \"embedding_dims\":embedding_dims,\n",
    "        \"vocab_size\":vocab_size\n",
    "    }\n",
    "    source_positional_encoder_params = {\n",
    "        \"embedding_dims\": embedding_dims,\n",
    "        \"sequence_length\": sequence_length,\n",
    "        \"dropout\": dropout\n",
    "    }\n",
    "    encoder_params = {\n",
    "        \"embedding_dims\": embedding_dims,\n",
    "        \"num_encoders\": num_decoders,\n",
    "        \"expansion_dims\": expansion_dims,\n",
    "        \"source_mask\": None,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"dropout\": dropout\n",
    "    }\n",
    "    target_embedding_layer_params = {\n",
    "        \"embedding_dims\":embedding_dims,\n",
    "        \"vocab_size\":vocab_size\n",
    "    }\n",
    "    target_positional_encoder_params = {\n",
    "        \"embedding_dims\": embedding_dims,\n",
    "        \"sequence_length\": sequence_length,\n",
    "        \"dropout\": dropout\n",
    "    }\n",
    "    decoder_params = {\n",
    "        \"num_decoders\": num_decoders,\n",
    "        \"embedding_dims\": embedding_dims,\n",
    "        \"expansion_dims\": expansion_dims,\n",
    "        \"target_mask\": None,\n",
    "        \"source_mask\": None,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"dropout\": dropout\n",
    "    }\n",
    "    output_layer_params = {\n",
    "        \"embedding_dims\":embedding_dims,\n",
    "        \"vocab_size\":vocab_size\n",
    "    }\n",
    "\n",
    "\n",
    "    transformer = Transformer(source_embedding_layer_params, source_positional_encoder_params, encoder_params, target_embedding_layer_params, target_positional_encoder_params, decoder_params, output_layer_params)\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1: \n",
    "            nn.init.xavier_uniform_(p) #Xavier uniform random initiation\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (source_embedding_layer): EmbeddingLayer(\n",
       "    (embedding_layer): Embedding(10000, 512)\n",
       "  )\n",
       "  (source_positional_encoder): PositionalEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderBlock(\n",
       "        (multi_head_attention_block): MultiHeadAttention(\n",
       "          (W_queries): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_keys): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_values): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm_1): LayerNormalizer()\n",
       "        (feed_foward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm_2): LayerNormalizer()\n",
       "      )\n",
       "    )\n",
       "    (layer_normalizer): LayerNormalizer()\n",
       "  )\n",
       "  (target_embedding_layer): EmbeddingLayer(\n",
       "    (embedding_layer): Embedding(10000, 512)\n",
       "  )\n",
       "  (target_positional_encoder): PositionalEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (masked_self_attention_block): MultiHeadAttention(\n",
       "          (W_queries): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_keys): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_values): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm_1): LayerNormalizer()\n",
       "        (cross_attention_block): MultiHeadAttention(\n",
       "          (W_queries): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_keys): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_values): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm_2): LayerNormalizer()\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm_3): LayerNormalizer()\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNormalizer()\n",
       "  )\n",
       "  (output_layer): OutputLayer(\n",
       "    (linear_layer): Linear(in_features=512, out_features=10000, bias=True)\n",
       "    (log_softmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
